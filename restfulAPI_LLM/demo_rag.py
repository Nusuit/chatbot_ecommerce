# -*- coding: utf-8 -*-
"""Demo_RAG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PDvVl6ihE7eCsYB0v84Ks83x-4qjPYC9

**Data Processing**
"""

# from google.colab import drive
# drive.mount('/content/drive')

# file_path = '/content/drive/MyDrive/export_csv/flipkart_com-ecommerce_sample.csv'

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q datasets

from datasets import load_dataset

ds = load_dataset("TrainingDataPro/asos-e-commerce-dataset")

import pandas as pd
df = pd.DataFrame(ds['train'])
df.head()

df.drop_duplicates(subset='name', keep='first', inplace=True)

df.dropna(inplace=True)

df

"""**Vector Indexing**"""

# Commented out IPython magic to ensure Python compatibility.
# # Import thư viện langchain và pinecone
# %%capture
# %pip install -qU langchain-pinecone pinecone-notebooks
# %pip install -qU langchain-huggingface
# %pip install -qU langchain-nomic
# %pip install -q langchain_community
# %pip install -q sentence-transformers
# !pip install pinecone-client

from google.colab import userdata
import os
import time

from pinecone import Pinecone, ServerlessSpec

if not os.getenv("huykien283Pinecone"):
    os.environ["huykien283Pinecone"] = userdata.get('huykien283Pinecone')

pinecone_api_key = os.environ.get("huykien283Pinecone")

pc = Pinecone(api_key=pinecone_api_key)

"""**Initialize Pinecone**"""

# Import thư viện time để dùng cho việc dừng chương trình trong khoảng thời gian ngắn
import time

# Tên của index, có thể thay đổi tùy ý
index_name = "langchain-rag"

# Lấy danh sách các index hiện có từ Pinecone và lưu tên của chúng vào danh sách existing_indexes
existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]

# Kiểm tra nếu index có tên index_name chưa tồn tại trong existing_indexes
if index_name not in existing_indexes:
    # Tạo index mới với các thông số như tên index, số chiều (dimension) và metric để tính toán độ tương đồng
    pc.create_index(
        name=index_name,
        dimension=384,  # Định nghĩa số chiều của vector dữ liệu
        metric="cosine",  # Metric dùng để tính toán độ tương đồng cosine giữa các vector
        spec=ServerlessSpec(cloud="aws", region="us-east-1"),  # Sử dụng serverless spec, định nghĩa cloud và vùng dữ liệu
    )
    # Kiểm tra trạng thái index, đợi cho đến khi index sẵn sàng
    while not pc.describe_index(index_name).status["ready"]:
        time.sleep(1)  # Dừng chương trình trong 1 giây trước khi kiểm tra lại

# Kết nối đến index đã tạo hoặc đã tồn tại
pinecone_index = pc.Index(index_name)

# Import thư viện uuid để tạo các ID ngẫu nhiên duy nhất
from uuid import uuid4
# Import class Document từ langchain_core để tạo các tài liệu
from langchain_core.documents import Document

docs = []  # Khởi tạo danh sách trống để chứa các Document

# Lặp qua từng dòng dữ liệu trong DataFrame 'data'
for i, row in df.iterrows():
    # Tạo đối tượng Document và thêm vào danh sách 'docs'
    docs.append(Document(
        page_content=row['name'],  # Đặt nội dung của tài liệu từ cột 'product_name'
        # Thêm 1 số tham số khác bổ sung cho page_content
        metadata={
            'url': row['url'],
            'size': row['size'],
            'category': row['category'],
            'price': row['price'],
            'color': row['color'],
            'sku': row['sku'],
            'description': row['description'],
            'images': row['images']
        }
    ))

# In ra số lượng tài liệu đã tạo
print(len(docs))

# In ra tài liệu đầu tiên trong danh sách 'docs' để kiểm tra
print(docs[0])

"""**Embedding**"""

from uuid import uuid4
from tqdm import tqdm
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_pinecone import PineconeVectorStore
from langchain_community.embeddings import HuggingFaceEmbeddings
from google.colab import userdata

def add_documents_to_pinecone(docs, index_name, batch_size=100):
    # Khởi tạo embedding model
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    # Kết nối đến Pinecone index
    pinecone_index = Pinecone(api_key=userdata.get("huykien283Pinecone")).Index(index_name)
    vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)

    # Tiền xử lý và chia nhỏ tài liệu
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(docs)

    # Thêm tài liệu theo batch
    for i in tqdm(range(0, len(chunks), batch_size), desc="Adding documents"):
        batch = chunks[i:i+batch_size]
        try:
            vector_store.add_documents(
                documents=batch,
                ids=[str(uuid4()) for _ in range(len(batch))]
            )
        except Exception as e:
            print(f"Lỗi khi xử lý batch {i}: {e}")

    print(f"Đã thêm {len(chunks)} chunks vào Pinecone index.")

# Sử dụng hàm
add_documents_to_pinecone(docs, "langchain-test-index")

from huggingface_hub import login
from langchain.chains import RetrievalQA
from google.colab import userdata
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from google.colab import userdata

# Get API
login(token=userdata.get('gemma2b'))

# Load Tokenizer and Model LLM
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-3b")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-3b")

# Move the model to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""**Test LLM**"""

# def ask_gemma(prompt, max_new_tokens=50):
#     if tokenizer.pad_token is None:
#         # Use the end-of-sequence token as the padding token
#         tokenizer.pad_token = tokenizer.eos_token

#     # Sử dụng tokenizer đã được định nghĩa trước (giả sử được cấu hình chính xác)
#     inputs = tokenizer(prompt, return_tensors="pt", padding=True)

#     # Chuyển các tensor sang thiết bị (GPU hoặc CPU)
#     input_ids = inputs.input_ids.to(device)
#     attention_mask = inputs.attention_mask.to(device)

#     # Chuyển model sang thiết bị giống với input_ids
#     model.to(device)  # Đảm bảo mô hình cũng nằm trên cùng một thiết bị

#     # Tạo câu trả lời với số lượng token tối đa
#     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens)

#     # Giải mã kết quả, loại bỏ các token đặc biệt
#     return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# # Sử dụng hàm ask_gemma
# prompt = "What is the president of the USA today?"
# generated_text = ask_gemma(prompt)
# print(generated_text)

from pinecone import Index  # Đúng cách import Index từ pinecone
from langchain.vectorstores import Pinecone as PineconeVectorStore  # Đúng cách import PineconeVectorStore từ langchain
from langchain.embeddings import HuggingFaceEmbeddings  # Để khởi tạo embeddings
from langchain.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from google.colab import userdata
from langchain_community.llms import CTransformers

# Cau hinh
model_id = "bigscience/bloom-3b" # Use a model id string instead of the model object
model = AutoModelForCausalLM.from_pretrained(model_id)

def load_llm(model_id):
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.01
    )
    return pipe


# Đọc từ Pinecone DB
def read_vectors_db():
    # Embedding model từ HuggingFace
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    model_kwargs = {'device': 'cpu', 'trust_remote_code': True}
    encode_kwargs = {'normalize_embeddings': False}

    # Khởi tạo HuggingFaceEmbeddings với mô hình Nomic Embed và API key
    embedding_model = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    # Kết nối với chỉ mục Pinecone
    pinecone_index = Index(api_key=userdata.get('huykien283Pinecone'), host='https://langchain-rag-hjlnxhm.svc.aped-4627-b74a.pinecone.io')

    # Tạo PineconeVectorStore
    vector_store = PineconeVectorStore(index=pinecone_index, embedding=embedding_model.embed_query, text_key="text")

    return vector_store

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-huggingface

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFacePipeline
from transformers import pipeline
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# Đọc từ Pinecone DB
def read_vectors_db():
    # Embedding model từ HuggingFace
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    embedding_model = HuggingFaceEmbeddings(model_name=model_name)

    # Kết nối với chỉ mục Pinecone
    pinecone_index = pc.Index(index_name)

    # Tạo PineconeVectorStore
    vector_store = PineconeVectorStore(index=pinecone_index, embedding=embedding_model, text_key="text")

    return vector_store

# Tạo QA Chain
def create_qa_chain(prompt, llm, db):
    base_retriever = db.as_retriever(search_kwargs={"k": 3})
    # Bỏ qua phần compression retriever
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=base_retriever,  # Truy vấn trực tiếp từ retriever
        return_source_documents=True,
        chain_type_kwargs={'prompt': prompt}
    )


# Phân tích câu hỏi của người dùng để hiểu loại câu hỏi
def analyze_question(question):
    if "price" in question.lower():
        return "price"
    elif "color" in question.lower():
        return "color"
    elif "size" in question.lower() or "available sizes" in question.lower():
        return "size"
    elif "stock" in question.lower() or "available" in question.lower():
        return "availability"
    else:
        return "general"

# Xử lý kết quả truy vấn từ Pinecone
def process_query_results(documents, query_type):
    relevant_info = []

    for doc in documents:
        # Trả về thông tin dựa trên loại câu hỏi người dùng (query_type)
        if query_type == "price" and 'price' in doc.metadata:
            relevant_info.append({
                'category': doc.metadata.get('category', ''),
                'price': doc.metadata.get('price', 'Không có thông tin')
            })
        elif query_type == "color" and 'color' in doc.metadata:
            relevant_info.append({
                'category': doc.metadata.get('category', ''),
                'color': doc.metadata.get('color', 'Không có thông tin')
            })
        elif query_type == "size" and 'size' in doc.metadata:
            relevant_info.append({
                'category': doc.metadata.get('category', ''),
                'size': doc.metadata.get('size', 'Không có thông tin')
            })
        elif query_type == "availability" and 'stock' in doc.metadata:
            relevant_info.append({
                'category': doc.metadata.get('category', ''),
                'stock': doc.metadata.get('stock', 'Không có thông tin')
            })
        else:
            relevant_info.append({
                'category': doc.metadata.get('category', 'Không có thông tin'),
                'general_info': doc.metadata  # Trả về toàn bộ metadata khi không có thông tin cụ thể
            })

    return relevant_info

# Tóm tắt kết quả truy vấn
def summarize_results(relevant_info, query_type):
    if not relevant_info:
        return "Tôi không có đủ thông tin để trả lời câu hỏi đó một cách chính xác."

    if len(relevant_info) == 1:
        item = relevant_info[0]
        if query_type == "price":
            return f"Giá của {item['category']} là {item['price']}."
        elif query_type == "color":
            return f"Màu sắc của {item['category']} là {item['color']}."
        elif query_type == "size":
            return f"Kích thước có sẵn của {item['category']} là {item['size']}."
        elif query_type == "availability":
            return f"Trạng thái kho của {item['category']} là {item['stock']}."
        else:
            return f"Thông tin chung của {item['category']} là {item['general_info']}."
    else:
        summary = "Tôi tìm thấy một số sản phẩm phù hợp:\n"
        for item in relevant_info:
            if query_type == "price":
                summary += f"- {item['category']}: {item['price']}\n"
            elif query_type == "color":
                summary += f"- {item['category']}: {item['color']}\n"
            elif query_type == "size":
                summary += f"- {item['category']}: {item['size']}\n"
            elif query_type == "availability":
                summary += f"- {item['category']}: {item['stock']}\n"
            else:
                summary += f"- {item['category']}: {item['general_info']}\n"
        return summary

# Hàm thực thi truy vấn
def run_query(chain, question):
    response = chain({"query": question})

    print(f"Đã lấy ra {len(response['source_documents'])} tài liệu từ Pinecone.")
    for i, doc in enumerate(response['source_documents'], 1):
        print(f"\nTài liệu {i}:")
        print(f"Nội dung: {doc.page_content[:200]}...")
        print(f"Metadata: {doc.metadata}")

    # Phân tích câu hỏi để hiểu loại câu hỏi
    query_type = analyze_question(question)

    # Xử lý kết quả dựa trên loại câu hỏi
    relevant_info = process_query_results(response['source_documents'], query_type)

    # Tóm tắt kết quả
    summary = summarize_results(relevant_info, query_type)

    return summary


# Tạo prompt template
def create_prompt():
    template = """You are a helpful assistant for an e-commerce website. Your task is to answer questions about products based on the provided context. Answer only using the information in the context provided. You must not add or guess any information not present in the context.

    Instructions:
    1. Provide a concise and accurate answer based **only** on the context provided.
    2. If the exact information is not available in the context, say "I don't have enough information to answer that question accurately."
    3. If the question is about the price, and the 'price' field is present in the context, use it directly in your answer.
    4. If the question is about available sizes, colors, or stock, and those fields are present, refer to them directly.
    5. If the required field is not in the context, clearly state that the information is not available, without adding or assuming any details.
    6. Always ensure your answer is aligned with the structure and information in the 'context' provided, even if it's incomplete.

    Context:  {context}

    Question: {question}

    Answer:
    """

    return PromptTemplate(template=template, input_variables=["context", "question"])


# Cập nhật hàm setup_llm
def setup_llm():
    model_name = "bigscience/bloom-3b"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

    # Cấu hình pipeline với các tham số phù hợp
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,  # Số lượng token tối đa cho output
        temperature=0.01,
        no_repeat_ngram_size=2
    )

    # Tạo HuggingFacePipeline với các tham số tùy chỉnh
    llm = HuggingFacePipeline(
        pipeline=pipe,
        model_kwargs={"temperature": 0.01, "max_length": 512}
    )
    return llm

# Cập nhật hàm main để bao gồm xử lý lỗi
def main():
    try:
        db = read_vectors_db()
        prompt = create_prompt()
        llm = setup_llm()
        qa_chain = create_qa_chain(prompt, llm, db)

        question = "What is the "'price'" of New Look trench coat in camel?"
        response = run_query(qa_chain, question)
        print("Câu trả lời:", response)

    except ValueError as e:
        print(f"Gặp lỗi: {e}")
        print("Vui lòng kiểm tra lại cấu hình của mô hình và độ dài của input.")

if __name__ == "__main__":
    main()

# Cập nhật hàm main để bao gồm xử lý lỗi
def main():
    try:
        db = read_vectors_db()
        prompt = create_prompt()
        llm = setup_llm()
        qa_chain = create_qa_chain(prompt, llm, db)

        question = "I want to buy a coat, my budget is 60.00, Can you recommend me some coat suitable?"
        response = run_query(qa_chain, question)
        print("Câu trả lời:", response)

    except ValueError as e:
        print(f"Gặp lỗi: {e}")
        print("Vui lòng kiểm tra lại cấu hình của mô hình và độ dài của input.")

if __name__ == "__main__":
    main()

# Cập nhật hàm main để bao gồm xử lý lỗi
def main():
    try:
        db = read_vectors_db()
        prompt = create_prompt()
        llm = setup_llm()
        qa_chain = create_qa_chain(prompt, llm, db)

        question = "What is the color of ASOS DESIGN Curve canvas oversized bomber jacket in stone?"
        response = run_query(qa_chain, question)
        print("Câu trả lời:", response)

    except ValueError as e:
        print(f"Gặp lỗi: {e}")
        print("Vui lòng kiểm tra lại cấu hình của mô hình và độ dài của input.")

if __name__ == "__main__":
    main()

df

